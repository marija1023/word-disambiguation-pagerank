{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import semcor\n",
    "from nltk.corpus.reader.semcor import SemcorCorpusReader\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_FILES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsd():\n",
    "    avg_acc = 0\n",
    "    files = random.sample(semcor.fileids(), NUM_OF_FILES)\n",
    "    for file in files:\n",
    "        clean_words, original_to_lemma = preprocess(file)\n",
    "        word_senses = dict([(w, None) for w in clean_words])\n",
    "        \n",
    "        G = create_graph(clean_words, word_senses)\n",
    "        \n",
    "        pr = nx.pagerank(G)\n",
    "        \n",
    "        avg_acc += evaluate(pr, word_senses, original_to_lemma, file)\n",
    "    \n",
    "    avg_acc /= NUM_OF_FILES\n",
    "    print('Average wsd accuracy: {} achieved on {} files'.format(avg_acc, NUM_OF_FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file):\n",
    "    reader = SemcorCorpusReader(semcor.root, semcor.fileids(), wn)\n",
    "    \n",
    "    words = reader.words(file)\n",
    "    \n",
    "    # pos tagging\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    # convert to wordnet pos tag\n",
    "    tagged_words = list(map(lambda pair: (pair[0], to_wordnet_pos(pair[1][0])), tagged_words))\n",
    "    # filter out those without wordnet tag\n",
    "    tagged_words = list(filter(lambda pair: pair[1] != None, tagged_words))\n",
    "    \n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w, tag) for (w, tag) in tagged_words]\n",
    "    \n",
    "    # map from original word to lemmatized\n",
    "    original_to_lemma = dict([(w, lemmatizer.lemmatize(w, tag)) for (w, tag) in tagged_words])\n",
    "    \n",
    "    # stopwords\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    punctuation = string.punctuation\n",
    "    normalized_words = map(lambda x: x.lower(), lemmatized_words)\n",
    "    clean_words = list(filter(lambda x: x not in english_stopwords and x not in punctuation, normalized_words))\n",
    "    \n",
    "    # collocations\n",
    "    wn_collocations(clean_words, original_to_lemma)\n",
    "    \n",
    "    #remove duplicates - order doesn't matter to pagerank\n",
    "    clean_words = list(set(clean_words))\n",
    "    \n",
    "    return clean_words, original_to_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_wordnet_pos(nltk_pos):\n",
    "    if nltk_pos == \"J\":\n",
    "        return wn.ADJ\n",
    "    elif nltk_pos == \"N\":\n",
    "        return wn.NOUN\n",
    "    elif nltk_pos == \"V\":\n",
    "        return wn.VERB\n",
    "    elif nltk_pos == \"R\":\n",
    "        return wn.ADV\n",
    "    elif nltk_pos == \"S\":\n",
    "        return wn.ADJ_SAT\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_collocations(words, original_to_lemma):\n",
    "    max_col_size = 5\n",
    "    for col_size in range(max_col_size, 1, -1):\n",
    "        i = 0\n",
    "        while i <= len(words) - col_size:\n",
    "            col = \"_\".join([words[j] for j in range(i, i + col_size)])\n",
    "            if len(wn.synsets(col)) != 0:               \n",
    "                # ???\n",
    "                original_to_lemma[words[i]] = col\n",
    "                \n",
    "                words[i] = col\n",
    "                for j in range(i + col_size - 1, i, -1):\n",
    "                    original_to_lemma[words[j]] = 'collocation: ' + col\n",
    "                    del words[j]\n",
    "                \n",
    "#                 print(col, len(wn.synsets(col)))\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(clean_words, word_senses):\n",
    "    G = nx.Graph()\n",
    "    # create nodes\n",
    "    for w in clean_words:\n",
    "        # TODO: try adding pos when getting synsets\n",
    "        w_synsets = wn.synsets(w)\n",
    "        if len(w_synsets) > 1:\n",
    "            for synset in w_synsets:\n",
    "                G.add_node((w, synset))\n",
    "        elif len(w_synsets) == 1:\n",
    "            word_senses[w] = w_synsets[0]\n",
    "    #     if there are no synsets - leave None\n",
    "    #                 G.add_node(synset.name())\n",
    "    \n",
    "    # create edges\n",
    "    for node_i in G.nodes:\n",
    "        for node_j in G.nodes:\n",
    "            if node_i[0] != node_j[0] and are_connected(node_i[1], node_j[1]):\n",
    "                G.add_edge(node_i, node_j)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_connected(u, v):\n",
    "    if u in v.hypernyms() or u in v.hyponyms() or u in v.part_holonyms() or u in v.part_meronyms() or u in v.substance_holonyms() or u in v.substance_meronyms():\n",
    "        return True\n",
    "#     coordinate relation - e.g. wolf and dog\n",
    "    u_hyper = set(u.hypernyms())\n",
    "    v_hyper = set(v.hypernyms())\n",
    "    if len(u_hyper & v_hyper) > 0:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pr, word_senses, original_to_lemma, file):\n",
    "    max_values = dict([(w, 0.0) for w in word_senses.keys()])\n",
    "    \n",
    "    # for each word assign sense as the max value from pagerank\n",
    "    for k, v in pr.items():\n",
    "        if v > max_values[k[0]]:\n",
    "            word_senses[k[0]] = k[1]\n",
    "            max_values[k[0]] = v\n",
    "    \n",
    "    # get actual meaning of words from original semcor file\n",
    "    actual_meaning = get_actual_meaning(file)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = calculate_accuracy(original_to_lemma, word_senses, actual_meaning)\n",
    "    print('Accuracy: {}'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_meaning(file):\n",
    "    actual_meaning = {}\n",
    "    no_lemma_sense = 0\n",
    "    trees = list(semcor.tagged_chunks(fileids=file, tag='sem'))\n",
    "    for tree in trees:\n",
    "        if isinstance(tree, nltk.tree.Tree):\n",
    "            actual_meaning['_'.join(tree.leaves())] = tree.label()\n",
    "            if not isinstance(tree.label(), nltk.corpus.reader.wordnet.Lemma):\n",
    "                #TODO: handle these - create Lemma (or Synset) from string if possible...\n",
    "                # There are mistakes in semcor file: e.g. toe.a.00 sense doesn't exist in wordnet\n",
    "                no_lemma_sense += 1\n",
    "    print('Number of words that do not have lemma sense in semcor file: {}'.format(no_lemma_sense))\n",
    "    \n",
    "    return actual_meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(original_to_lemma, word_senses, actual_meaning):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    no_word_sense = 0\n",
    "    num_collocations = 0\n",
    "    no_actual_meaning = 0\n",
    "    for k, v in original_to_lemma.items():\n",
    "        v_l = v.lower()\n",
    "        if v_l in word_senses.keys() and k in actual_meaning.keys():\n",
    "    #         print('original: {}, sense: {}'.format(k, word_senses[v]))\n",
    "            if isinstance(actual_meaning[k], nltk.corpus.reader.wordnet.Lemma) and word_senses[v_l] is not None and actual_meaning[k] in word_senses[v_l].lemmas():\n",
    "                correct += 1\n",
    "    #             print('correct')\n",
    "            else:\n",
    "                incorrect +=1\n",
    "    #             print('incorrect')\n",
    "        elif v_l not in word_senses.keys():\n",
    "    #         print('no word sense for {} : {}'.format(k, v))\n",
    "            if 'collocation: ' in v_l:\n",
    "                num_collocations += 1\n",
    "            else:\n",
    "                no_word_sense += 1\n",
    "        else:\n",
    "#             print('{} not in actual meaning keys'.format(k))\n",
    "            no_actual_meaning += 1\n",
    "\n",
    "    print('Correct: {}, incorrect: {}, num_collocations: {}, no_word_sense: {}, no_actual_meaning: {}'.format(correct, incorrect, num_collocations, no_word_sense, no_actual_meaning))\n",
    "    \n",
    "    return correct / (correct + incorrect)\n",
    "#     ili \n",
    "#     return correct / (correct + incorrect + no_word_sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that do not have lemma sense in semcor file: 65\n",
      "Correct: 573, incorrect: 82, num_collocations: 15, no_word_sense: 51, no_actual_meaning: 60\n",
      "Accuracy: 0.8748091603053435\n",
      "Number of words that do not have lemma sense in semcor file: 49\n",
      "Correct: 390, incorrect: 59, num_collocations: 22, no_word_sense: 42, no_actual_meaning: 51\n",
      "Accuracy: 0.8685968819599109\n",
      "Number of words that do not have lemma sense in semcor file: 77\n",
      "Correct: 382, incorrect: 57, num_collocations: 20, no_word_sense: 34, no_actual_meaning: 48\n",
      "Accuracy: 0.8701594533029613\n",
      "Number of words that do not have lemma sense in semcor file: 43\n",
      "Correct: 304, incorrect: 54, num_collocations: 20, no_word_sense: 37, no_actual_meaning: 45\n",
      "Accuracy: 0.8491620111731844\n",
      "Number of words that do not have lemma sense in semcor file: 3\n",
      "Correct: 147, incorrect: 9, num_collocations: 22, no_word_sense: 51, no_actual_meaning: 512\n",
      "Accuracy: 0.9423076923076923\n",
      "Average wsd accuracy: 0.8810070398098185 achieved on 5 files\n"
     ]
    }
   ],
   "source": [
    "wsd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('refuse', 'NN')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['refuse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('they', 'PRP'), ('refuse', 'VBP')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['they', 'refuse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos tagging depends on context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('are.n.01'),\n",
       " Synset('be.v.01'),\n",
       " Synset('be.v.02'),\n",
       " Synset('be.v.03'),\n",
       " Synset('exist.v.01'),\n",
       " Synset('be.v.05'),\n",
       " Synset('equal.v.01'),\n",
       " Synset('constitute.v.01'),\n",
       " Synset('be.v.08'),\n",
       " Synset('embody.v.02'),\n",
       " Synset('be.v.10'),\n",
       " Synset('be.v.11'),\n",
       " Synset('be.v.12'),\n",
       " Synset('cost.v.01')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('are') # there are stopwords in wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('russell.n.07')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bertrand_russell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('russell.n.07.Russell'),\n",
       " Lemma('russell.n.07.Bertrand_Russell'),\n",
       " Lemma('russell.n.07.Bertrand_Arthur_William_Russell'),\n",
       " Lemma('russell.n.07.Earl_Russell')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bertrand_russell')[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = ['I', 'am', 'Bertrand', 'Arthur', 'William', 'Russell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_collocations(test_words, pam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'Bertrand_Arthur_William_Russell']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'easy_money'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_to_lemma['easy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'collocation: easy_money'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_to_lemma['money']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"n't\") # => errors in semcor file ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
