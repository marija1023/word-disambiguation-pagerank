{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Word sense disambiguation** - assign the most appropriate meaning to a polysemous word within a given context  \n",
    "Example:  \n",
    "*I can hear bass sound.*  \n",
    "*He likes to eat grilled bass.*  \n",
    "  \n",
    "2. **WordNet** - lexical database of semantic relations between words  \n",
    "Words are grouped into synsets (sets of synonyms) that express a distinct concept  \n",
    "Relations between synsets:  \n",
    "hyperonymy/hyponymy (IS_A relation) - furniture, bed  \n",
    "meronymy/holonymy (part of relation) - finger, hand  \n",
    "  \n",
    "3. **PageRank algorithm** - rank web pages  \n",
    "Main idea - a lot of important web pages have links to an important web page\n",
    "  \n",
    "Combination of all 3:\n",
    "- Create a graph from words based on their WordNet relations\n",
    "- Apply PageRank algorithm to that graph\n",
    "- Assign meaning to each polysemous word based on the rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "6t6rLN75UVJe",
    "outputId": "ec8ce124-caf1-4b6c-a008-f198c85ea922"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package semcor to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/stefan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('semcor')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import semcor\n",
    "from nltk.corpus.reader.semcor import SemcorCorpusReader\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: \"Sizzling\", meaning: (Lemma('sizzling.s.01.sizzling') Sizzling)\n",
      "word: \"temperatures\", meaning: (Lemma('temperature.n.01.temperature') temperatures)\n",
      "word: \"and\", meaning: ['and']\n",
      "word: \"hot\", meaning: (Lemma('hot.a.01.hot') hot)\n",
      "word: \"summer\", meaning: (Lemma('summer.n.01.summer') summer)\n",
      "word: \"pavements\", meaning: (Lemma('pavement.n.01.pavement') pavements)\n",
      "word: \"are\", meaning: (Lemma('be.v.01.be') are)\n",
      "word: \"anything\", meaning: ['anything']\n",
      "word: \"but\", meaning: ['but']\n",
      "word: \"kind\", meaning: (kind.s.00 kind)\n",
      "word: \"to\", meaning: ['to']\n",
      "word: \"the\", meaning: ['the']\n",
      "word: \"feet\", meaning: (Lemma('foot.n.01.foot') feet)\n",
      "word: \".\", meaning: ['.']\n",
      "word: \"That\", meaning: ['That']\n",
      "word: \"is\", meaning: ['is']\n",
      "word: \"why\", meaning: ['why']\n",
      "word: \"it\", meaning: ['it']\n",
      "word: \"is\", meaning: (Lemma('be.v.01.be') is)\n",
      "word: \"important\", meaning: (Lemma('important.a.01.important') important)\n",
      "word: \"to\", meaning: ['to']\n",
      "word: \"invest\", meaning: (Lemma('invest.v.01.invest') invest)\n",
      "word: \"in\", meaning: ['in']\n",
      "word: \"comfortable\", meaning: (Lemma('comfortable.a.01.comfortable') comfortable)\n",
      "word: \",\", meaning: [',']\n",
      "word: \"airy\", meaning: (Lemma('aired.s.01.airy') airy)\n",
      "word: \"types\", meaning: (Lemma('type.n.01.type') types)\n",
      "word: \"of\", meaning: ['of']\n",
      "word: \"shoes\", meaning: (Lemma('shoe.n.01.shoe') shoes)\n",
      "word: \".\", meaning: ['.']\n"
     ]
    }
   ],
   "source": [
    "# example semcor file\n",
    "file = semcor.fileids()[7]\n",
    "reader = SemcorCorpusReader(semcor.root, semcor.fileids(), wn)\n",
    "words = reader.words(file)\n",
    "senses = reader.tagged_chunks(file, 'sem')\n",
    "for i in range(30):\n",
    "    print('word: \"{}\", meaning: {}'.format(words[i], senses[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1sAbbdNUVJy"
   },
   "outputs": [],
   "source": [
    "NUM_OF_FILES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRRN_3FnUVKC"
   },
   "outputs": [],
   "source": [
    "# main function for word sense dismabiguation\n",
    "def wsd():\n",
    "    avg_acc = 0\n",
    "    files = random.sample(semcor.fileids(), NUM_OF_FILES)\n",
    "#     files = [semcor.fileids()[7]]\n",
    "    print(files)\n",
    "    for file in files:\n",
    "        clean_words, original_to_lemma = preprocess(file)\n",
    "        word_senses = dict([(w, None) for w in clean_words])\n",
    "        \n",
    "        G = create_graph(clean_words, word_senses)\n",
    "        \n",
    "        pr = nx.pagerank(G)\n",
    "        \n",
    "        avg_acc += evaluate(pr, word_senses, original_to_lemma, file)\n",
    "    \n",
    "    avg_acc /= NUM_OF_FILES\n",
    "    print('Average wsd accuracy: {} achieved on {} files'.format(avg_acc, NUM_OF_FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Gd5WbaPUVKO"
   },
   "outputs": [],
   "source": [
    "# preprocessing of the semcor file - pos tagging, lemmatization, stopwords removal, collocations identification\n",
    "def preprocess(file):\n",
    "    reader = SemcorCorpusReader(semcor.root, semcor.fileids(), wn)\n",
    "    \n",
    "    words = reader.words(file)\n",
    "    \n",
    "    # pos tagging\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    # convert to wordnet pos tag\n",
    "    tagged_words = list(map(lambda pair: (pair[0], to_wordnet_pos(pair[1][0])), tagged_words))\n",
    "    # filter out those without wordnet tag\n",
    "    tagged_words = list(filter(lambda pair: pair[1] != None, tagged_words))\n",
    "    \n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w, tag) for (w, tag) in tagged_words]\n",
    "    \n",
    "    # map from original word to lemmatized\n",
    "    original_to_lemma = {(w.lower(), i) : lemmatizer.lemmatize(w, tag).lower() for (i, (w, tag)) in enumerate(tagged_words)}\n",
    "\n",
    "    \n",
    "    \n",
    "    # stopwords\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    punctuation = string.punctuation\n",
    "    normalized_words = map(lambda x: x.lower(), lemmatized_words)\n",
    "    clean_words = list(filter(lambda x: x not in english_stopwords and x not in punctuation, normalized_words))\n",
    "    \n",
    "    original_to_lemma = {k : v for k, v in original_to_lemma.items() if v not in english_stopwords and v not in punctuation}\n",
    "\n",
    "\n",
    "    # collocations\n",
    "    clean_words, original_to_lemma = wn_collocations(clean_words, original_to_lemma)\n",
    "\n",
    "    \n",
    "    #remove duplicates - order doesn't matter to pagerank\n",
    "    clean_words = list(set(clean_words))\n",
    "    \n",
    "    return clean_words, original_to_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-MTtvaWSUVKY"
   },
   "outputs": [],
   "source": [
    "# convert nltk pos tag to wordnet's pos tag\n",
    "def to_wordnet_pos(nltk_pos):\n",
    "    if nltk_pos == \"J\":\n",
    "        return wn.ADJ\n",
    "    elif nltk_pos == \"N\":\n",
    "        return wn.NOUN\n",
    "    elif nltk_pos == \"V\":\n",
    "        return wn.VERB\n",
    "    elif nltk_pos == \"R\":\n",
    "        return wn.ADV\n",
    "    elif nltk_pos == \"S\":\n",
    "        return wn.ADJ_SAT\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sPX85gNGUVKj"
   },
   "outputs": [],
   "source": [
    "# find collocations - sequence of words that forms a compound concept defined in wordnet\n",
    "def wn_collocations(words, original_to_lemma):\n",
    "    # for every col_size group of adjacent words check if they are a collocation\n",
    "    # i.e. there is a meaning in wordnet for the group\n",
    "    max_col_size = 5\n",
    "    for col_size in range(max_col_size, 1, -1):\n",
    "        to_delete = []\n",
    "        to_delete_o2l = []\n",
    "        cols_to_add = []\n",
    "        cols_to_add_ind = []\n",
    "        for (k, ix), v in original_to_lemma.items():\n",
    "            if ix > len(words) - col_size:\n",
    "                break\n",
    "            if ix in to_delete:\n",
    "                continue\n",
    "            clean_col = \"_\".join([words[j] for j in range(ix, ix + col_size)])\n",
    "            orig_col = \"_\".join([k for (k, index) in original_to_lemma.keys() if index in range(ix, ix + col_size)])\n",
    "\n",
    "            # if there is a meaning in wordnet - add collocation as a new 'word' and delete individual words\n",
    "            # perhaps too complicated way to do that, but if it's stupid and it works...\n",
    "            if len(wn.synsets(clean_col)) != 0:\n",
    "                cols_to_add.append((orig_col, clean_col))\n",
    "                cols_to_add_ind.append(ix)\n",
    "                \n",
    "                to_delete_o2l.append(ix)\n",
    "                words[ix] = clean_col\n",
    "\n",
    "                for j in range(ix + col_size - 1, ix, -1):\n",
    "                    to_delete.append(j)\n",
    "                    to_delete_o2l.append(j)\n",
    "                    \n",
    "        new_indexes = list(range(len(words) - len(to_delete_o2l)))\n",
    "        new_or_to_lemma = {}\n",
    "        new_ix = 0\n",
    "        help_i = 0\n",
    "        for (k, ix), v in original_to_lemma.items():\n",
    "            if ix not in to_delete_o2l:\n",
    "                new_or_to_lemma[(k, new_ix)] = v\n",
    "                new_ix += 1\n",
    "            elif ix in cols_to_add_ind:\n",
    "                new_or_to_lemma[(cols_to_add[help_i][0], new_ix)] = cols_to_add[help_i][1]\n",
    "                help_i += 1\n",
    "                new_ix += 1\n",
    "\n",
    "        original_to_lemma = new_or_to_lemma\n",
    "        words = [w for (i, w) in enumerate(words) if i not in to_delete]\n",
    "\n",
    "    original_to_lemma = {k : v for ((k, ix), v) in original_to_lemma.items()}\n",
    "\n",
    "\n",
    "    return words, original_to_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xOVOxnxUVKt"
   },
   "outputs": [],
   "source": [
    "# create graph for pagerank\n",
    "# wordnet synsets are vertices\n",
    "# edge exists between two vertices if there is a wordnet relation (e.g. hypernym/hyponym) between those synsets\n",
    "def create_graph(clean_words, word_senses):\n",
    "    G = nx.Graph()\n",
    "    # create nodes\n",
    "    for w in clean_words:\n",
    "        # TODO: try adding pos when getting synsets\n",
    "        w_synsets = wn.synsets(w)\n",
    "        if len(w_synsets) > 1:\n",
    "            for synset in w_synsets:\n",
    "                G.add_node((w, synset))\n",
    "        elif len(w_synsets) == 1:\n",
    "            # if there is only one meaning in wordnet, we choose that one\n",
    "            word_senses[w] = w_synsets[0]\n",
    "    #     if there are no synsets - leave None\n",
    "    #                 G.add_node(synset.name())\n",
    "    \n",
    "    # create edges\n",
    "    for node_i in G.nodes:\n",
    "        for node_j in G.nodes:\n",
    "            if node_i[0] != node_j[0] and are_connected(node_i[1], node_j[1]):\n",
    "                G.add_edge(node_i, node_j)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAKR83XUUVK0"
   },
   "outputs": [],
   "source": [
    "# determine if two synsets are connected in wordnet\n",
    "# hypernym/hyponym, holonym/meronym, coordinate (mutual hypernym)\n",
    "def are_connected(u, v):\n",
    "    if u in v.hypernyms() or u in v.hyponyms() or u in v.part_holonyms() or u in v.part_meronyms() or u in v.substance_holonyms() or u in v.substance_meronyms():\n",
    "        return True\n",
    "#     coordinate relation - e.g. wolf and dog\n",
    "#     + this will connect two different words that are in the same synset = we think this should be done \n",
    "#     it is not specified in the original paper\n",
    "    u_hyper = set(u.hypernyms())\n",
    "    v_hyper = set(v.hypernyms())\n",
    "    if len(u_hyper & v_hyper) > 0:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adoPdlf2UVK8"
   },
   "outputs": [],
   "source": [
    "# compare the pagerank senses with original senses from semcor file \n",
    "def evaluate(pr, word_senses, original_to_lemma, file):\n",
    "    max_values = dict([(w, 0.0) for w in word_senses.keys()])\n",
    "    \n",
    "    # for each word assign sense as the max value from pagerank\n",
    "    for k, v in pr.items():\n",
    "        if v > max_values[k[0]]:\n",
    "            word_senses[k[0]] = k[1]\n",
    "            max_values[k[0]] = v\n",
    "    \n",
    "    # get actual meaning of words from original semcor file\n",
    "    actual_meaning = get_actual_meaning(file)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = calculate_accuracy(original_to_lemma, word_senses, actual_meaning, show_errors=True)\n",
    "    print('Accuracy: {}'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aoQlCykoUVLE"
   },
   "outputs": [],
   "source": [
    "# each word in semcor file has a meaning (wordnet synset) assigned by an expert  \n",
    "def get_actual_meaning(file):\n",
    "    actual_meaning = {}\n",
    "    no_lemma_sense = 0\n",
    "    trees = list(semcor.tagged_chunks(fileids=file, tag='sem'))\n",
    "    no_trees = 0\n",
    "    for tree in trees:\n",
    "        if isinstance(tree, nltk.tree.Tree):\n",
    "            actual_meaning['_'.join(tree.leaves()).lower()] = tree.label()\n",
    "            if not isinstance(tree.label(), nltk.corpus.reader.wordnet.Lemma):\n",
    "                #TODO: handle these - create Lemma (or Synset) from string if possible...\n",
    "                # There are mistakes in semcor file: e.g. toe.a.00 sense doesn't exist in wordnet\n",
    "                no_lemma_sense += 1\n",
    "        else:\n",
    "          no_trees += 1\n",
    "    print('Number of words that do not have lemma sense in semcor file: {}'.format(no_lemma_sense))\n",
    "    print('Number of words that do not have tree in semcor file: {}'.format(no_trees))\n",
    "    \n",
    "    return actual_meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1sBceKrUVLN"
   },
   "outputs": [],
   "source": [
    "# calculate the fraction of words that our algorithm assigned the same sense as in semcor file\n",
    "def calculate_accuracy(original_to_lemma, word_senses, actual_meaning, show_errors=False):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    broken_meaning = 0\n",
    "#     print(actual_meaning)\n",
    "    error_log = []\n",
    "    \n",
    "    for k, v in actual_meaning.items():\n",
    "        if not isinstance(v, nltk.corpus.reader.wordnet.Lemma):\n",
    "#             print(\"Broken meaning in the semcor file for: {}\".format(k))\n",
    "            broken_meaning += 1\n",
    "        elif k not in original_to_lemma.keys():\n",
    "            if len(wn.synsets(k)) != 0:\n",
    "                # they have a collocation that exists in wordnet, but we don't\n",
    "                # probably contains stopwords...\n",
    "#                 print(\"They found a collocation, and we didn't: {}\".format(k))\n",
    "                incorrect += 1\n",
    "                error_log.append((k, None, v))\n",
    "        elif original_to_lemma[k] not in word_senses.keys():\n",
    "            print(\"This shouldn't happen: {}\".format(k))\n",
    "        elif word_senses[original_to_lemma[k]] is None:\n",
    "            # probably the word doesn't exist in wordnet...\n",
    "#             print(\"They have a meaning, and we don't: {}\".format(k))\n",
    "            incorrect += 1\n",
    "            error_log.append((k, None, v))\n",
    "        elif v in word_senses[original_to_lemma[k]].lemmas():\n",
    "            # we chose the right meaning\n",
    "            correct += 1\n",
    "        else:\n",
    "            # we chose the wrong meaning\n",
    "            incorrect += 1\n",
    "            error_log.append((k, word_senses[original_to_lemma[k]], v))\n",
    "\n",
    "\n",
    "    if show_errors:\n",
    "        for word, our_sense, their_sense in error_log:\n",
    "            print('Word: \"{}\", our sense: {}, their sense: {}'.format(word, our_sense, their_sense))\n",
    "            \n",
    "    print(\"Correct = {}, incorrect = {}\".format(correct, incorrect))\n",
    "    \n",
    "    return correct / (correct + incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LI5_W7ksUVLW",
    "outputId": "aec84a0a-e89c-47c3-9646-3cf2bbe16eab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('nehru.n.01')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('jawaharlal_nehru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wceSGtWFUVLf",
    "outputId": "9f978765-502f-4cd5-fd15-8eb6d61282bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown1/tagfiles/br-g11.xml', 'brown1/tagfiles/br-a02.xml', 'brownv/tagfiles/br-e14.xml', 'brown2/tagfiles/br-p24.xml', 'brown1/tagfiles/br-k28.xml']\n",
      "Number of words that do not have lemma sense in semcor file: 43\n",
      "Number of words that do not have tree in semcor file: 1099\n",
      "Word: \"more\", our sense: None, their sense: Lemma('more.r.01.more')\n",
      "Word: \"then\", our sense: None, their sense: Lemma('then.r.02.then')\n",
      "Word: \"will\", our sense: None, their sense: Lemma('will.n.02.will')\n",
      "Word: \"cope_with\", our sense: None, their sense: Lemma('meet.v.06.cope_with')\n",
      "Word: \"is\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"of_course\", our sense: None, their sense: Lemma('naturally.r.01.of_course')\n",
      "Word: \"just\", our sense: None, their sense: Lemma('precisely.r.01.just')\n",
      "Word: \"other\", our sense: None, their sense: Lemma('other.a.01.other')\n",
      "Word: \"not\", our sense: None, their sense: Lemma('not.r.01.not')\n",
      "Word: \"lavoisier\", our sense: Synset('lavoisier.n.01'), their sense: Lemma('person.n.01.person')\n",
      "Word: \"archimedes\", our sense: Synset('archimedes.n.01'), their sense: Lemma('person.n.01.person')\n",
      "Word: \"two\", our sense: None, their sense: Lemma('two.s.01.two')\n",
      "Word: \"most\", our sense: None, their sense: Lemma('most.r.01.most')\n",
      "Word: \"have\", our sense: None, their sense: Lemma('have.v.01.have')\n",
      "Word: \"down\", our sense: None, their sense: Lemma('down.r.01.down')\n",
      "Word: \"are\", our sense: None, their sense: Lemma('exist.v.01.be')\n",
      "Word: \"physics\", our sense: Synset('purgative.n.01'), their sense: Lemma('physics.n.01.physics')\n",
      "Word: \"haunting\", our sense: Synset('frequent.v.02'), their sense: Lemma('haunting.s.01.haunting')\n",
      "Word: \"copernicus\", our sense: Synset('copernicus.n.01'), their sense: Lemma('person.n.01.person')\n",
      "Word: \"no\", our sense: None, their sense: Lemma('no.a.01.no')\n",
      "Word: \"increased\", our sense: Synset('increase.v.02'), their sense: Lemma('increased.a.01.increased')\n",
      "Word: \"socialized\", our sense: Synset('socialize.v.04'), their sense: Lemma('socialized.s.01.socialized')\n",
      "Word: \"a_few\", our sense: None, their sense: Lemma('a_few.s.01.a_few')\n",
      "Word: \"means\", our sense: Synset('mean.v.01'), their sense: Lemma('means.n.01.means')\n",
      "Word: \"only\", our sense: None, their sense: Lemma('merely.r.01.only')\n",
      "Word: \"so\", our sense: None, their sense: Lemma('so.r.01.so')\n",
      "Word: \"very_much\", our sense: None, their sense: Lemma('a_lot.r.01.very_much')\n",
      "Word: \"in_fact\", our sense: None, their sense: Lemma('in_fact.r.01.in_fact')\n",
      "Word: \"day_by_day\", our sense: None, their sense: Lemma('day_by_day.r.01.day_by_day')\n",
      "Word: \"lucretius\", our sense: Synset('lucretius.n.01'), their sense: Lemma('person.n.01.person')\n",
      "Word: \"were\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"worse\", our sense: Synset('bad.n.01'), their sense: Lemma('worse.a.01.worse')\n",
      "Word: \"now\", our sense: None, their sense: Lemma('nowadays.r.01.now')\n",
      "Word: \"was\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"same\", our sense: None, their sense: Lemma('same.a.02.same')\n",
      "Word: \"once\", our sense: None, their sense: Lemma('once.r.03.once')\n",
      "Word: \"worried\", our sense: Synset('worry.v.02'), their sense: Lemma('disquieted.s.01.worried')\n",
      "Word: \"do\", our sense: None, their sense: Lemma('make.v.01.do')\n",
      "Word: \"push\", our sense: None, their sense: Lemma('push.v.01.push')\n",
      "Word: \"button\", our sense: None, their sense: Lemma('push_button.n.01.button')\n",
      "Word: \"am\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"stand_out\", our sense: None, their sense: Lemma('excel.v.01.stand_out')\n",
      "Word: \"in_general\", our sense: None, their sense: Lemma('generally.r.02.in_general')\n",
      "Word: \"been\", our sense: None, their sense: Lemma('be.v.05.be')\n",
      "Word: \"be\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"detailed\", our sense: Synset('detail.v.02'), their sense: Lemma('detailed.s.01.detailed')\n",
      "Word: \"illuminating\", our sense: Synset('light.v.01'), their sense: Lemma('enlightening.a.01.illuminating')\n",
      "Word: \"and_then\", our sense: None, their sense: Lemma('then.r.01.and_then')\n",
      "Word: \"account\", our sense: Synset('explanation.n.01'), their sense: Lemma('account_for.v.01.account_for')\n",
      "Word: \"very\", our sense: None, their sense: Lemma('very.r.01.very')\n",
      "Word: \"no_longer\", our sense: None, their sense: Lemma('no_longer.r.01.no_longer')\n",
      "Word: \"in_common\", our sense: None, their sense: Lemma('in_common.r.01.in_common')\n",
      "Word: \"think_of\", our sense: None, their sense: Lemma('think_of.v.03.think_of')\n",
      "Word: \"own\", our sense: None, their sense: Lemma('own.s.01.own')\n",
      "Word: \"at_least\", our sense: None, their sense: Lemma('at_least.r.01.at_least')\n",
      "Word: \"here\", our sense: None, their sense: Lemma('here.r.02.here')\n",
      "Word: \"has\", our sense: None, their sense: Lemma('experience.v.03.have')\n",
      "Word: \"up\", our sense: None, their sense: Lemma('up.r.01.up')\n",
      "Word: \"cross\", our sense: None, their sense: Lemma('traverse.v.01.cross')\n",
      "Word: \"street\", our sense: None, their sense: Lemma('street.n.01.street')\n",
      "Word: \"drunk\", our sense: Synset('drink.v.02'), their sense: Lemma('drunkard.n.01.drunk')\n",
      "Word: \"drunks\", our sense: Synset('drink.v.02'), their sense: Lemma('drunkard.n.01.drunk')\n",
      "Word: \"carrying_out\", our sense: None, their sense: Lemma('follow_through.v.02.carry_out')\n",
      "Word: \"in_toto\", our sense: None, their sense: Lemma('in_toto.r.01.in_toto')\n",
      "Word: \"talk_about\", our sense: None, their sense: Lemma('discourse.v.01.talk_about')\n",
      "Word: \"international\", our sense: Synset('international.n.01'), their sense: Lemma('international.a.01.international')\n",
      "Word: \"three\", our sense: None, their sense: Lemma('three.s.01.three')\n",
      "Word: \"19th\", our sense: None, their sense: Lemma('nineteenth.s.01.19th')\n",
      "Word: \"atomic_physics\", our sense: None, their sense: Lemma('nuclear_physics.n.01.atomic_physics')\n",
      "Word: \"bio\", our sense: None, their sense: Lemma('biological.a.01.biological')\n",
      "Word: \"lie_in\", our sense: None, their sense: Lemma('dwell.v.02.lie_in')\n",
      "Word: \"interesting\", our sense: Synset('matter_to.v.01'), their sense: Lemma('interesting.a.01.interesting')\n",
      "Word: \"for_example\", our sense: None, their sense: Lemma('for_example.r.01.for_example')\n",
      "Word: \"continued\", our sense: Synset('continue.v.10'), their sense: Lemma('continued.a.01.continued')\n",
      "Word: \"established\", our sense: Synset('lay_down.v.01'), their sense: Lemma('conventional.s.02.established')\n",
      "Word: \"having\", our sense: None, their sense: Lemma('give_birth.v.01.have')\n",
      "Word: \"recent\", our sense: Synset('holocene.n.01'), their sense: Lemma('recent.s.01.recent')\n",
      "Word: \"x-rays\", our sense: Synset('x-ray.v.01'), their sense: Lemma('x_ray.n.01.X_ray')\n",
      "Word: \"in_this\", our sense: None, their sense: Lemma('therein.r.01.in_this')\n",
      "Correct = 573, incorrect = 79\n",
      "Accuracy: 0.8788343558282209\n",
      "Number of words that do not have lemma sense in semcor file: 28\n",
      "Number of words that do not have tree in semcor file: 1002\n",
      "Word: \"abandoned\", our sense: Synset('wildness.n.01'), their sense: Lemma('abandoned.s.01.abandoned')\n",
      "Word: \"daniel\", our sense: Synset('daniel.n.01'), their sense: Lemma('person.n.01.person')\n",
      "Word: \"two\", our sense: None, their sense: Lemma('two.s.01.two')\n",
      "Word: \"hearing\", our sense: Synset('hear.v.01'), their sense: Lemma('hearing.n.01.hearing')\n",
      "Word: \"one\", our sense: None, their sense: Lemma('one.s.01.one')\n",
      "Word: \"17\", our sense: None, their sense: Lemma('seventeen.s.01.17')\n",
      "Word: \"million\", our sense: None, their sense: Lemma('million.n.01.million')\n",
      "Word: \"anticipated\", our sense: Synset('anticipate.v.05'), their sense: Lemma('anticipated.s.01.anticipated')\n",
      "Word: \"next\", our sense: None, their sense: Lemma('future.s.03.next')\n",
      "Word: \"aug.\", our sense: None, their sense: Lemma('august.n.01.Aug')\n",
      "Word: \"means\", our sense: Synset('mean.v.01'), their sense: Lemma('means.n.01.means')\n",
      "Word: \"been\", our sense: None, their sense: Lemma('be.v.03.be')\n",
      "Word: \"was\", our sense: None, their sense: Lemma('be.v.03.be')\n",
      "Word: \"take_over\", our sense: None, their sense: Lemma('bear.v.06.take_over')\n",
      "Word: \"other\", our sense: None, their sense: Lemma('other.a.01.other')\n",
      "Word: \"seven\", our sense: None, their sense: Lemma('seven.s.01.seven')\n",
      "Word: \"more\", our sense: None, their sense: Lemma('more.a.02.more')\n",
      "Word: \"now\", our sense: None, their sense: Lemma('nowadays.r.01.now')\n",
      "Word: \"is\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"tyler\", our sense: Synset('tyler.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"do\", our sense: None, their sense: Lemma('make.v.01.do')\n",
      "Word: \"sherman\", our sense: Synset('sherman.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"insurance\", our sense: Synset('policy.n.03'), their sense: Lemma('insurance_company.n.01.insurance_company')\n",
      "Word: \"harlingen\", our sense: None, their sense: Lemma('location.n.01.location')\n",
      "Word: \"cox\", our sense: Synset('cyclooxygenase.n.01'), their sense: Lemma('person.n.01.person')\n",
      "Word: \"not\", our sense: None, their sense: Lemma('not.r.01.not')\n",
      "Word: \"only\", our sense: None, their sense: Lemma('merely.r.01.only')\n",
      "Word: \"four\", our sense: None, their sense: Lemma('four.s.01.four')\n",
      "Word: \"schooling\", our sense: Synset('school.n.02'), their sense: Lemma('schooling.n.01.schooling')\n",
      "Word: \"reduced\", our sense: Synset('reduce.v.20'), their sense: Lemma('reduced.s.02.reduced')\n",
      "Word: \"no\", our sense: None, their sense: Lemma('no.a.01.no')\n",
      "Word: \"6\", our sense: None, their sense: Lemma('six.s.01.6')\n",
      "Word: \"13\", our sense: None, their sense: Lemma('thirteen.s.01.13')\n",
      "Word: \"of_age\", our sense: None, their sense: Lemma('aged.s.03.of_age')\n",
      "Word: \"here\", our sense: None, their sense: Lemma('here.r.01.here')\n",
      "Word: \"five\", our sense: None, their sense: Lemma('five.s.01.five')\n",
      "Word: \"harris\", our sense: Synset('harris.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"bexar\", our sense: None, their sense: Lemma('location.n.01.location')\n",
      "Word: \"tarrant\", our sense: None, their sense: Lemma('location.n.01.location')\n",
      "Word: \"el_paso\", our sense: Synset('el_paso.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"be\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"savings\", our sense: Synset('salvage.v.01'), their sense: Lemma('economy.n.04.saving')\n",
      "Word: \"parkhouse\", our sense: None, their sense: Lemma('person.n.01.person')\n",
      "Word: \"tea\", our sense: Synset('tea.n.01'), their sense: Lemma('group.n.01.group')\n",
      "Word: \"saving\", our sense: Synset('salvage.v.01'), their sense: Lemma('rescue.n.01.saving')\n",
      "Word: \"details\", our sense: Synset('detail.n.01'), their sense: Lemma('details.n.01.details')\n",
      "Word: \"ratcliff\", our sense: None, their sense: Lemma('person.n.01.person')\n",
      "Word: \"berry\", our sense: Synset('berry.n.01'), their sense: Lemma('person.n.01.person')\n",
      "Word: \"were\", our sense: None, their sense: Lemma('be.v.02.be')\n",
      "Word: \"better\", our sense: Synset('well.n.01'), their sense: Lemma('better.r.01.better')\n",
      "Word: \"willing\", our sense: Synset('bequeath.v.01'), their sense: Lemma('willing.a.01.willing')\n",
      "Word: \"on_it\", our sense: None, their sense: Lemma('thereon.r.01.on_it')\n",
      "Word: \"san_antonio\", our sense: Synset('san_antonio.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"planning\", our sense: Synset('plan.v.02'), their sense: Lemma('planning.n.01.planning')\n",
      "Word: \"three\", our sense: None, their sense: Lemma('three.n.01.three')\n",
      "Word: \"galveston\", our sense: Synset('galveston.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"mentally_retarded\", our sense: None, their sense: Lemma('mentally_retarded.n.01.mentally_retarded')\n",
      "Word: \"gulf_coast\", our sense: Synset('gulf_coast.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"later_on\", our sense: None, their sense: Lemma('subsequently.r.01.later_on')\n",
      "Word: \"in_the_meantime\", our sense: None, their sense: Lemma('meanwhile.r.02.in_the_meantime')\n",
      "Word: \"1000\", our sense: None, their sense: Lemma('thousand.s.01.1000')\n",
      "Word: \"paris\", our sense: Synset('paris.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"own\", our sense: None, their sense: Lemma('own.s.01.own')\n",
      "Word: \"12\", our sense: None, their sense: Lemma('twelve.s.01.12')\n",
      "Word: \"public_utility\", our sense: None, their sense: Lemma('utility.n.01.public_utility')\n",
      "Word: \"companies\", our sense: None, their sense: Lemma('company.n.01.company')\n",
      "Word: \"plainview\", our sense: None, their sense: Lemma('location.n.01.location')\n",
      "Word: \"congress\", our sense: Synset('congress.n.01'), their sense: Lemma('group.n.01.group')\n",
      "Word: \"formby\", our sense: None, their sense: Lemma('person.n.01.person')\n",
      "Word: \"speaker\", our sense: Synset('speaker.n.01'), their sense: Lemma('speaker.n.03.Speaker')\n",
      "Word: \"4\", our sense: None, their sense: Lemma('four.n.01.4')\n",
      "Word: \"seminole\", our sense: Synset('seminole.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"feb.\", our sense: None, their sense: Lemma('february.n.01.Feb')\n",
      "Word: \"22\", our sense: None, their sense: Lemma('twenty-two.n.01.22')\n",
      "Word: \"march\", our sense: Synset('march.n.03'), their sense: Lemma('march.n.01.March')\n",
      "Word: \"1\", our sense: None, their sense: Lemma('one.n.01.1')\n",
      "Word: \"roberts\", our sense: Synset('roberts.n.01'), their sense: Lemma('person.n.01.person')\n",
      "Word: \"paradise\", our sense: Synset('eden.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"alleged\", our sense: Synset('allege.v.01'), their sense: Lemma('alleged.s.02.alleged')\n",
      "Word: \"weatherford\", our sense: None, their sense: Lemma('location.n.01.location')\n",
      "Word: \"house_of_representatives\", our sense: None, their sense: Lemma('united_states_house_of_representatives.n.01.House_of_Representatives')\n",
      "Word: \"shouting\", our sense: Synset('shout.v.02'), their sense: Lemma('cheering.n.01.shouting')\n",
      "Word: \"most\", our sense: None, their sense: Lemma('most.r.01.most')\n",
      "Word: \"cotten\", our sense: None, their sense: Lemma('person.n.01.person')\n",
      "Word: \"veiled\", our sense: Synset('veil.v.01'), their sense: Lemma('veiled.a.01.veiled')\n",
      "Word: \"less\", our sense: Synset('lupus_erythematosus.n.01'), their sense: Lemma('less.a.01.less')\n",
      "Word: \"small-town\", our sense: None, their sense: Lemma('village.n.01.small_town')\n",
      "Word: \"dumas\", our sense: Synset('duma.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"mission\", our sense: Synset('deputation.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"newton\", our sense: Synset('newton.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"poor\", our sense: None, their sense: Lemma('poor.a.03.poor')\n",
      "Word: \"boy\", our sense: None, their sense: Lemma('boy.n.02.boy')\n",
      "Word: \"chapman\", our sense: Synset('chapman.n.01'), their sense: Lemma('person.n.01.person')\n",
      "Word: \"24\", our sense: None, their sense: Lemma('twenty-four.s.01.24')\n",
      "Word: \"teaching_methods\", our sense: None, their sense: Lemma('teaching_method.n.01.teaching_method')\n",
      "Word: \"junior\", our sense: Synset('junior.n.03'), their sense: Lemma('junior_high_school.n.01.junior_high')\n",
      "Word: \"teaching_certificate\", our sense: None, their sense: Lemma('teaching_certificate.n.01.teaching_certificate')\n",
      "Word: \"30\", our sense: None, their sense: Lemma('thirty.s.01.30')\n",
      "Word: \"grover\", our sense: None, their sense: Lemma('person.n.01.person')\n",
      "Word: \"have\", our sense: None, their sense: Lemma('own.v.01.have')\n",
      "Word: \"at_least\", our sense: None, their sense: Lemma('at_least.r.02.at_least')\n",
      "Word: \"physics\", our sense: Synset('purgative.n.01'), their sense: Lemma('physics.n.01.physics')\n",
      "Word: \"co-signers\", our sense: None, their sense: Lemma('cosigner.n.01.cosigner')\n",
      "Word: \"board_of_regents\", our sense: None, their sense: Lemma('board_of_regents.n.01.board_of_regents')\n",
      "Word: \"hays\", our sense: Synset('hay.v.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"kan.\", our sense: None, their sense: Lemma('location.n.01.location')\n",
      "Word: \"earned\", our sense: Synset('earn.v.02'), their sense: Lemma('earned.a.01.earned')\n",
      "Word: \"doctor_of_education\", our sense: None, their sense: Lemma('doctor_of_education.n.01.Doctor_of_Education')\n",
      "Word: \"master_of_science\", our sense: None, their sense: Lemma('master_of_science.n.01.Master_of_Science')\n",
      "Word: \"bachelor_of_science\", our sense: None, their sense: Lemma('bachelor_of_science.n.01.Bachelor_of_Science')\n",
      "Word: \"okla.\", our sense: None, their sense: Lemma('location.n.01.location')\n",
      "Word: \"massachusetts_institute_of_technology\", our sense: None, their sense: Lemma('group.n.01.group')\n",
      "Word: \"u._s._army\", our sense: Synset('united_states_army.n.01'), their sense: Lemma('group.n.01.group')\n",
      "Word: \"re-elected\", our sense: None, their sense: Lemma('reelect.v.01.reelect')\n",
      "Word: \"federal\", our sense: Synset('federal.n.01'), their sense: Lemma('federal.a.02.federal')\n",
      "Word: \"racial\", our sense: None, their sense: Lemma('racial.a.01.racial')\n",
      "Word: \"blue_ribbon\", our sense: Synset('blue_ribbon.n.01'), their sense: Lemma('blue-ribbon.s.01.blue-ribbon')\n",
      "Word: \"citizens_committee\", our sense: None, their sense: Lemma('committee.n.02.citizens_committee')\n",
      "Word: \"adc\", our sense: None, their sense: Lemma('group.n.01.group')\n",
      "Word: \"cook\", our sense: Synset('cook.v.02'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"consulting_firm\", our sense: None, their sense: Lemma('consulting_firm.n.01.consulting_firm')\n",
      "Word: \"10\", our sense: None, their sense: Lemma('ten.s.01.10')\n",
      "Word: \"soaring\", our sense: Synset('soar.n.01'), their sense: Lemma('soaring.s.01.soaring')\n",
      "Word: \"depend_upon\", our sense: None, their sense: Lemma('depend_on.v.01.depend_upon')\n",
      "Word: \"martin\", our sense: Synset('martin.n.01'), their sense: Lemma('person.n.01.person')\n",
      "Word: \"limited\", our sense: Synset('restrict.v.03'), their sense: Lemma('limited.a.01.limited')\n",
      "Word: \"are\", our sense: None, their sense: Lemma('be.v.02.be')\n",
      "Word: \"underlying\", our sense: Synset('underlie.v.01'), their sense: Lemma('implicit_in.s.01.underlying')\n",
      "Word: \"projects\", our sense: None, their sense: Lemma('undertaking.n.01.project')\n",
      "Correct = 475, incorrect = 129\n",
      "Accuracy: 0.7864238410596026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that do not have lemma sense in semcor file: 0\n",
      "Number of words that do not have tree in semcor file: 1946\n",
      "Word: \"are\", our sense: None, their sense: Lemma('be.v.05.be')\n",
      "Word: \"rest_on\", our sense: None, their sense: Lemma('lean_on.v.01.rest_on')\n",
      "Word: \"have\", our sense: None, their sense: Lemma('get.v.03.have')\n",
      "Word: \"be\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"does\", our sense: None, their sense: Lemma('do.v.04.do')\n",
      "Word: \"having\", our sense: None, their sense: Lemma('get.v.03.have')\n",
      "Word: \"is\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"has\", our sense: None, their sense: Lemma('have.v.02.have')\n",
      "Word: \"do\", our sense: None, their sense: Lemma('do.v.04.do')\n",
      "Word: \"put\", our sense: Synset('put.v.01'), their sense: Lemma('work.v.12.put_to_work')\n",
      "Word: \"turn_off\", our sense: None, their sense: Lemma('turn_off.v.02.turn_off')\n",
      "Word: \"pull_out\", our sense: None, their sense: Lemma('pull_out.v.01.pull_out')\n",
      "Word: \"load_up\", our sense: None, their sense: Lemma('load.v.01.load_up')\n",
      "Word: \"pop_in\", our sense: None, their sense: Lemma('pop_in.v.01.pop_in')\n",
      "Word: \"think_of\", our sense: None, their sense: Lemma('remember.v.02.think_of')\n",
      "Word: \"come_upon\", our sense: None, their sense: Lemma('fall_upon.v.01.come_upon')\n",
      "Word: \"had\", our sense: None, their sense: Lemma('induce.v.02.have')\n",
      "Word: \"roll_out\", our sense: None, their sense: Lemma('roll_out.v.01.roll_out')\n",
      "Correct = 147, incorrect = 18\n",
      "Accuracy: 0.8909090909090909\n",
      "Number of words that do not have lemma sense in semcor file: 56\n",
      "Number of words that do not have tree in semcor file: 1397\n",
      "Word: \"was\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"riverside\", our sense: Synset('riverbank.n.01'), their sense: Lemma('location.n.01.location')\n",
      "Word: \"turned_out\", our sense: None, their sense: Lemma('turn_out.v.02.turn_out')\n",
      "Word: \"have\", our sense: None, their sense: Lemma('have.v.01.have')\n",
      "Word: \"a_few\", our sense: None, their sense: Lemma('a_few.s.01.a_few')\n",
      "Word: \"anniston\", our sense: None, their sense: Lemma('location.n.01.location')\n",
      "Word: \"figure\", our sense: Synset('figure.v.02'), their sense: Lemma('solve.v.01.figure_out')\n",
      "Word: \"then\", our sense: None, their sense: Lemma('then.r.01.then')\n",
      "Word: \"playing\", our sense: Synset('play.v.01'), their sense: Lemma('playing.n.02.playing')\n",
      "Word: \"cut\", our sense: Synset('cut.n.06'), their sense: Lemma('cut_short.v.02.cut_short')\n",
      "Word: \"eddie\", our sense: None, their sense: Lemma('person.n.01.person')\n",
      "Word: \"phil\", our sense: None, their sense: Lemma('person.n.01.person')\n",
      "Word: \"mike\", our sense: Synset('microphone.n.01'), their sense: Lemma('person.n.01.person')\n",
      "Word: \"just\", our sense: None, their sense: Lemma('just.r.04.just')\n",
      "Word: \"too\", our sense: None, their sense: Lemma('excessively.r.01.too')\n",
      "Word: \"down\", our sense: None, their sense: Lemma('down.r.01.down')\n",
      "Word: \"tossed\", our sense: Synset('toss.n.03'), their sense: Lemma('discard.v.01.toss_away')\n",
      "Word: \"now\", our sense: None, their sense: Lemma('now.r.03.now')\n",
      "Word: \"over\", our sense: None, their sense: Lemma('complete.s.05.over')\n",
      "Word: \"all_the_way\", our sense: None, their sense: Lemma('clear.r.01.all_the_way')\n",
      "Word: \"in_for\", our sense: None, their sense: Lemma('in_for.s.01.in_for')\n",
      "Word: \"not\", our sense: None, their sense: Lemma('not.r.01.not')\n",
      "Word: \"words\", our sense: Synset('give_voice.v.01'), their sense: Lemma('words.n.01.words')\n",
      "Word: \"so\", our sense: None, their sense: Lemma('so.r.01.so')\n",
      "Word: \"deegan\", our sense: None, their sense: Lemma('person.n.01.person')\n",
      "Word: \"alone\", our sense: None, their sense: Lemma('alone.r.02.alone')\n",
      "Word: \"own\", our sense: None, their sense: Lemma('own.s.01.own')\n",
      "Word: \"looking_for\", our sense: None, their sense: Lemma('search.v.01.look_for')\n",
      "Word: \"fit_in\", our sense: None, their sense: Lemma('harmonize.v.01.fit_in')\n",
      "Word: \"had\", our sense: None, their sense: Lemma('have.v.01.have')\n",
      "Word: \"do\", our sense: None, their sense: Lemma('make.v.01.do')\n",
      "Word: \"only\", our sense: None, their sense: Lemma('merely.r.01.only')\n",
      "Word: \"injured\", our sense: Synset('injure.v.01'), their sense: Lemma('injured.a.01.injured')\n",
      "Word: \"a_little\", our sense: None, their sense: Lemma('a_bit.r.01.a_little')\n",
      "Word: \"a_couple_of\", our sense: None, their sense: Lemma('a_few.s.01.a_couple_of')\n",
      "Word: \"other\", our sense: None, their sense: Lemma('other.a.01.other')\n",
      "Word: \"four\", our sense: None, their sense: Lemma('four.s.01.four')\n",
      "Word: \"men\", our sense: Synset('man.n.01'), their sense: Lemma('work_force.n.01.men')\n",
      "Word: \"in_front\", our sense: None, their sense: Lemma('ahead.r.01.in_front')\n",
      "Word: \"flushed\", our sense: Synset('flower.n.03'), their sense: Lemma('flushed.s.01.flushed')\n",
      "Word: \"pitched\", our sense: Synset('slope.v.01'), their sense: Lemma('pitched.a.01.pitched')\n",
      "Word: \"god\", our sense: Synset('idol.n.01'), their sense: Lemma('god.n.01.God')\n",
      "Word: \"guys\", our sense: Synset('guy.n.02'), their sense: Lemma('guy.n.01.guy')\n",
      "Word: \"again\", our sense: None, their sense: Lemma('again.r.01.again')\n",
      "Word: \"shouting\", our sense: Synset('abuse.v.03'), their sense: Lemma('cheering.n.01.shouting')\n",
      "Word: \"be\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"box\", our sense: Synset('box.n.01'), their sense: Lemma('batter's_box.n.01.batter's_box')\n",
      "Word: \"very\", our sense: None, their sense: Lemma('very.r.01.very')\n",
      "Word: \"wound_up\", our sense: None, their sense: Lemma('wind_up.v.02.wind_up')\n",
      "Word: \"crowded\", our sense: Synset('crowd.v.03'), their sense: Lemma('crowded.a.01.crowded')\n",
      "Word: \"screaming\", our sense: Synset('shout.v.02'), their sense: Lemma('scream.n.01.screaming')\n",
      "Word: \"two\", our sense: None, their sense: Lemma('two.s.01.two')\n",
      "Word: \"fighting\", our sense: Synset('fight.v.02'), their sense: Lemma('fight.n.02.fighting')\n",
      "Word: \"guy\", our sense: Synset('guy.n.02'), their sense: Lemma('guy.n.01.guy')\n",
      "Word: \"team-mate\", our sense: None, their sense: Lemma('teammate.n.01.teammate')\n",
      "Word: \"come_out\", our sense: None, their sense: Lemma('come_on.v.01.come_out')\n",
      "Word: \"near\", our sense: None, their sense: Lemma('near.a.01.near')\n",
      "Word: \"come_in\", our sense: None, their sense: Lemma('enter.v.01.come_in')\n",
      "Word: \"gang_up\", our sense: None, their sense: Lemma('gang.v.01.gang_up')\n",
      "Word: \"up\", our sense: None, their sense: Lemma('up.r.01.up')\n",
      "Word: \"and_then\", our sense: None, their sense: Lemma('then.r.01.and_then')\n",
      "Word: \"rossoff\", our sense: None, their sense: Lemma('person.n.01.person')\n",
      "Word: \"doing\", our sense: None, their sense: Lemma('make.v.01.do')\n",
      "Word: \"a_trifle\", our sense: None, their sense: Lemma('a_bit.r.01.a_trifle')\n",
      "Word: \"is\", our sense: None, their sense: Lemma('be.v.02.be')\n",
      "Word: \"all_along\", our sense: None, their sense: Lemma('all_along.r.01.all_along')\n",
      "Word: \"sit_down\", our sense: None, their sense: Lemma('sit_down.v.01.sit_down')\n",
      "Word: \"give_and_take\", our sense: None, their sense: Lemma('give_and_take.v.01.give_and_take')\n",
      "Word: \"here\", our sense: None, their sense: Lemma('here.r.01.here')\n",
      "Word: \"are\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"back_up\", our sense: None, their sense: Lemma('support.v.01.back_up')\n",
      "Word: \"dazed\", our sense: Synset('dazzle.v.01'), their sense: Lemma('dazed.s.01.dazed')\n",
      "Word: \"get_it\", our sense: None, their sense: Lemma('catch_on.v.01.get_it')\n",
      "Word: \"run_bases\", our sense: None, their sense: Lemma('run_bases.v.01.run_bases')\n",
      "Word: \"no\", our sense: None, their sense: Lemma('no.a.01.no')\n",
      "Word: \"did\", our sense: None, their sense: Lemma('make.v.01.do')\n",
      "Word: \"later_on\", our sense: None, their sense: Lemma('subsequently.r.01.later_on')\n",
      "Word: \"way_of_life\", our sense: None, their sense: Lemma('way.n.05.way_of_life')\n",
      "Word: \"were\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"felt\", our sense: Synset('felt.v.03'), their sense: Lemma('feel.v.07.feel')\n",
      "Word: \"glued\", our sense: Synset('glue.v.01'), their sense: Lemma('glued.s.01.glued')\n",
      "Word: \"dressed\", our sense: Synset('dress.v.09'), their sense: Lemma('appareled.s.01.dressed')\n",
      "Word: \"frankie\", our sense: None, their sense: Lemma('person.n.01.person')\n",
      "Word: \"stared\", our sense: Synset('star.v.01'), their sense: Lemma('gaze.v.01.stare')\n",
      "Word: \"another\", our sense: None, their sense: Lemma('another.s.01.another')\n",
      "Word: \"whitey\", our sense: Synset('whitey.n.01'), their sense: Lemma('person.n.01.person')\n",
      "Word: \"all_right\", our sense: None, their sense: Lemma('all_right.r.02.all_right')\n",
      "Correct = 332, incorrect = 87\n",
      "Accuracy: 0.7923627684964201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that do not have lemma sense in semcor file: 36\n",
      "Number of words that do not have tree in semcor file: 1399\n",
      "Word: \"winston\", our sense: None, their sense: Lemma('person.n.01.person')\n",
      "Word: \"better\", our sense: Synset('commodity.n.01'), their sense: Lemma('better.r.01.better')\n",
      "Word: \"do\", our sense: None, their sense: Lemma('make.v.01.do')\n",
      "Word: \"had\", our sense: None, their sense: Lemma('have.v.01.have')\n",
      "Word: \"girl\", our sense: None, their sense: Lemma('female_child.n.01.girl')\n",
      "Word: \"were\", our sense: None, their sense: Lemma('be.v.05.be')\n",
      "Word: \"was\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"just_now\", our sense: None, their sense: Lemma('just.r.03.just_now')\n",
      "Word: \"leona\", our sense: None, their sense: Lemma('person.n.01.person')\n",
      "Word: \"down\", our sense: None, their sense: Lemma('down.r.01.down')\n",
      "Word: \"not\", our sense: None, their sense: Lemma('not.r.01.not')\n",
      "Word: \"washed\", our sense: Synset('wash.v.04'), their sense: Lemma('washed.s.01.washed')\n",
      "Word: \"four\", our sense: None, their sense: Lemma('four.s.01.four')\n",
      "Word: \"fried_eggs\", our sense: None, their sense: Lemma('fried_egg.n.01.fried_egg')\n",
      "Word: \"five\", our sense: None, their sense: Lemma('five.s.01.five')\n",
      "Word: \"alone\", our sense: None, their sense: Lemma('alone.s.01.alone')\n",
      "Word: \"is\", our sense: None, their sense: Lemma('be.v.02.be')\n",
      "Word: \"left\", our sense: Synset('left_field.n.01'), their sense: Lemma('leave.v.02.leave')\n",
      "Word: \"every_which_way\", our sense: None, their sense: Lemma('randomly.r.01.every_which_way')\n",
      "Word: \"clothes\", our sense: Synset('dress.v.02'), their sense: Lemma('apparel.n.01.clothes')\n",
      "Word: \"all_over\", our sense: None, their sense: Lemma('all_over.r.01.all_over')\n",
      "Word: \"one\", our sense: None, their sense: Lemma('one.s.01.one')\n",
      "Word: \"here\", our sense: None, their sense: Lemma('here.r.01.here')\n",
      "Word: \"stockings\", our sense: Synset('stock.v.01'), their sense: Lemma('stocking.n.01.stocking')\n",
      "Word: \"manners\", our sense: Synset('manner.n.01'), their sense: Lemma('manners.n.01.manners')\n",
      "Word: \"then\", our sense: None, their sense: Lemma('then.r.03.then')\n",
      "Word: \"spread_out\", our sense: None, their sense: Lemma('string_out.v.01.spread_out')\n",
      "Word: \"be\", our sense: None, their sense: Lemma('be.v.02.be')\n",
      "Word: \"now\", our sense: None, their sense: Lemma('nowadays.r.01.now')\n",
      "Word: \"embarrassed\", our sense: Synset('obstruct.v.01'), their sense: Lemma('abashed.s.01.embarrassed')\n",
      "Word: \"pleased\", our sense: Synset('please.v.02'), their sense: Lemma('pleased.a.01.pleased')\n",
      "Word: \"at_least\", our sense: None, their sense: Lemma('at_least.r.01.at_least')\n",
      "Word: \"think_about\", our sense: None, their sense: Lemma('entertain.v.02.think_about')\n",
      "Word: \"all\", our sense: None, their sense: Lemma('all.a.01.all')\n",
      "Word: \"only\", our sense: None, their sense: Lemma('merely.r.01.only')\n",
      "Word: \"been\", our sense: None, their sense: Lemma('be.v.01.be')\n",
      "Word: \"married\", our sense: Synset('marry.v.01'), their sense: Lemma('married.a.01.married')\n",
      "Word: \"cut\", our sense: Synset('cut.v.24'), their sense: Lemma('interrupt.v.01.cut_off')\n",
      "Word: \"too\", our sense: None, their sense: Lemma('excessively.r.01.too')\n",
      "Word: \"to_begin_with\", our sense: None, their sense: Lemma('in_the_first_place.r.01.to_begin_with')\n",
      "Word: \"have\", our sense: None, their sense: Lemma('have.v.01.have')\n",
      "Word: \"running\", our sense: Synset('run.v.13'), their sense: Lemma('running.a.01.running')\n",
      "Word: \"for_a_while\", our sense: None, their sense: Lemma('awhile.r.01.for_a_while')\n",
      "Word: \"lay\", our sense: Synset('put.v.01'), their sense: Lemma('lie.v.01.lie')\n",
      "Word: \"doing\", our sense: None, their sense: Lemma('make.v.01.do')\n",
      "Word: \"a_few\", our sense: None, their sense: Lemma('a_few.s.01.a_few')\n",
      "Word: \"brushed\", our sense: Synset('brush.n.02'), their sense: Lemma('brushed.s.02.brushed')\n",
      "Word: \"start_out\", our sense: None, their sense: Lemma('depart.v.03.start_out')\n",
      "Word: \"enough\", our sense: None, their sense: Lemma('enough.r.01.enough')\n",
      "Word: \"once\", our sense: None, their sense: Lemma('once.r.01.once')\n",
      "Word: \"corduroys\", our sense: Synset('corduroy.n.02'), their sense: Lemma('cords.n.01.corduroys')\n",
      "Word: \"smoking_jackets\", our sense: None, their sense: Lemma('lounging_jacket.n.01.smoking_jacket')\n",
      "Word: \"pearl\", our sense: None, their sense: Lemma('bone.n.03.pearl')\n",
      "Word: \"look_out\", our sense: None, their sense: Lemma('watch.v.05.look_out')\n",
      "Word: \"drove\", our sense: None, their sense: Lemma('drive.v.01.drive')\n",
      "Word: \"for_the_first_time\", our sense: None, their sense: Lemma('first.r.02.for_the_first_time')\n",
      "Word: \"get_at\", our sense: None, their sense: Lemma('access.v.02.get_at')\n",
      "Word: \"take_down\", our sense: None, their sense: Lemma('lower.v.01.take_down')\n",
      "Word: \"think_of\", our sense: None, their sense: Lemma('remember.v.02.think_of')\n",
      "Word: \"keeping\", our sense: Synset('keep.v.03'), their sense: Lemma('prolong.v.02.keep_up')\n",
      "Word: \"there\", our sense: None, their sense: Lemma('there.r.01.there')\n",
      "Word: \"surprised\", our sense: Synset('surprise.v.01'), their sense: Lemma('surprised.a.01.surprised')\n",
      "Word: \"steps\", our sense: Synset('gradation.n.01'), their sense: Lemma('stairs.n.01.steps')\n",
      "Word: \"spike_out\", our sense: None, their sense: Lemma('spike.v.04.spike_out')\n",
      "Word: \"after\", our sense: None, their sense: Lemma('subsequently.r.01.after')\n",
      "Word: \"while\", our sense: None, their sense: Lemma('while.n.01.while')\n",
      "Word: \"dug\", our sense: None, their sense: Lemma('dig.v.02.dig')\n",
      "Word: \"writing\", our sense: Synset('compose.v.02'), their sense: Lemma('writing.n.04.writing')\n",
      "Word: \"gardening\", our sense: Synset('garden.v.01'), their sense: Lemma('gardening.n.01.gardening')\n",
      "Word: \"take_to\", our sense: None, their sense: Lemma('fancy.v.02.take_to')\n",
      "Word: \"binding\", our sense: Synset('bind.v.07'), their sense: Lemma('binding.n.02.binding')\n",
      "Word: \"sweet\", our sense: Synset('sweet.n.01'), their sense: Lemma('odoriferous.s.03.sweet')\n",
      "Word: \"possessed\", our sense: Synset('possess.v.01'), their sense: Lemma('obsessed.s.02.possessed')\n",
      "Word: \"lay_out\", our sense: None, their sense: Lemma('range.v.05.lay_out')\n",
      "Word: \"over\", our sense: None, their sense: Lemma('complete.s.05.over')\n",
      "Word: \"relieved\", our sense: Synset('still.v.03'), their sense: Lemma('alleviated.s.01.relieved')\n",
      "Word: \"evening\", our sense: Synset('even.v.02'), their sense: Lemma('evening.n.01.evening')\n",
      "Word: \"come_down\", our sense: None, their sense: Lemma('descend.v.01.come_down')\n",
      "Word: \"about\", our sense: None, their sense: Lemma('approximately.r.01.about')\n",
      "Word: \"a_little\", our sense: None, their sense: Lemma('a_bit.r.01.a_little')\n",
      "Word: \"walk_around\", our sense: None, their sense: Lemma('perambulate.v.02.walk_around')\n",
      "Word: \"look_like\", our sense: None, their sense: Lemma('look_like.v.01.look_like')\n",
      "Word: \"on_it\", our sense: None, their sense: Lemma('thereon.r.01.on_it')\n",
      "Correct = 473, incorrect = 83\n",
      "Accuracy: 0.8507194244604317\n",
      "Average wsd accuracy: 0.8398498961507533 achieved on 5 files\n"
     ]
    }
   ],
   "source": [
    "wsd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "43uLR1eeUVLq",
    "outputId": "fb9ace09-76e6-4bfe-c0e8-559f7a6a6fa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('sideline.n.01')]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collocations that contain stopwords\n",
    "wn.synsets('responsible_for')\n",
    "wn.synsets('out_of_bounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdFB_T0oUVLy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('eye-strain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLzENQdlUVL7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G53wQG1UVMC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRQE5OxCUVMJ"
   },
   "outputs": [],
   "source": [
    "# some testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBdhcl1dUVMO",
    "outputId": "b378c16d-abe4-4da9-b968-7490d9b75fda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('refuse', 'NN')]"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['refuse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVZQZ4v5UVMV",
    "outputId": "fb32cbd5-f406-4a0b-f79a-eb7d8e98bc8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('they', 'PRP'), ('refuse', 'VBP')]"
      ]
     },
     "execution_count": 123,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['they', 'refuse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2uPysq3RUVMb"
   },
   "outputs": [],
   "source": [
    "#pos tagging depends on context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ChzFYYKUVMl",
    "outputId": "14cbc4cf-4720-4da7-aed1-c1a433779651"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('are.n.01'),\n",
       " Synset('be.v.01'),\n",
       " Synset('be.v.02'),\n",
       " Synset('be.v.03'),\n",
       " Synset('exist.v.01'),\n",
       " Synset('be.v.05'),\n",
       " Synset('equal.v.01'),\n",
       " Synset('constitute.v.01'),\n",
       " Synset('be.v.08'),\n",
       " Synset('embody.v.02'),\n",
       " Synset('be.v.10'),\n",
       " Synset('be.v.11'),\n",
       " Synset('be.v.12'),\n",
       " Synset('cost.v.01')]"
      ]
     },
     "execution_count": 138,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('are') # there are stopwords in wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGOcFCt9UVM8",
    "outputId": "d7c3abc8-2ce8-4ce5-b844-81482d653976"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('russell.n.07')]"
      ]
     },
     "execution_count": 147,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bertrand_russell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tL8xPstdUVNB",
    "outputId": "e5ec12c6-fa78-4593-cadd-97fab0f4919b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('russell.n.07.Russell'),\n",
       " Lemma('russell.n.07.Bertrand_Russell'),\n",
       " Lemma('russell.n.07.Bertrand_Arthur_William_Russell'),\n",
       " Lemma('russell.n.07.Earl_Russell')]"
      ]
     },
     "execution_count": 148,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bertrand_russell')[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fq7vcUZgUVNI"
   },
   "outputs": [],
   "source": [
    "test_words = ['I', 'am', 'Bertrand', 'Arthur', 'William', 'Russell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EfMFTgo8UVNN"
   },
   "outputs": [],
   "source": [
    "pam = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztQhMTxsUVNT"
   },
   "outputs": [],
   "source": [
    "wn_collocations(test_words, pam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KAsNrDXZUVNa",
    "outputId": "24c35676-adc3-4522-8f6f-ac53a694e21e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'Bertrand_Arthur_William_Russell']"
      ]
     },
     "execution_count": 152,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7Gc7feiUVNq",
    "outputId": "27d05562-f244-4b25-fbf1-697297a0c4b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 199,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"n't\") # => errors in semcor file ???"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wsd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
