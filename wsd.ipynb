{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "6t6rLN75UVJe",
    "outputId": "ec8ce124-caf1-4b6c-a008-f198c85ea922"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package semcor to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/stefan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('semcor')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import semcor\n",
    "from nltk.corpus.reader.semcor import SemcorCorpusReader\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1sAbbdNUVJy"
   },
   "outputs": [],
   "source": [
    "NUM_OF_FILES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRRN_3FnUVKC"
   },
   "outputs": [],
   "source": [
    "def wsd():\n",
    "    avg_acc = 0\n",
    "    files = random.sample(semcor.fileids(), NUM_OF_FILES)\n",
    "#     files = [semcor.fileids()[7]]\n",
    "    print(files)\n",
    "    for file in files:\n",
    "        clean_words, original_to_lemma = preprocess(file)\n",
    "        word_senses = dict([(w, None) for w in clean_words])\n",
    "        \n",
    "        G = create_graph(clean_words, word_senses)\n",
    "        \n",
    "        pr = nx.pagerank(G)\n",
    "        \n",
    "        avg_acc += evaluate(pr, word_senses, original_to_lemma, file)\n",
    "    \n",
    "    avg_acc /= NUM_OF_FILES\n",
    "    print('Average wsd accuracy: {} achieved on {} files'.format(avg_acc, NUM_OF_FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Gd5WbaPUVKO"
   },
   "outputs": [],
   "source": [
    "def preprocess(file):\n",
    "    reader = SemcorCorpusReader(semcor.root, semcor.fileids(), wn)\n",
    "    \n",
    "    words = reader.words(file)\n",
    "    \n",
    "    # pos tagging\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    # convert to wordnet pos tag\n",
    "    tagged_words = list(map(lambda pair: (pair[0], to_wordnet_pos(pair[1][0])), tagged_words))\n",
    "    # filter out those without wordnet tag\n",
    "    tagged_words = list(filter(lambda pair: pair[1] != None, tagged_words))\n",
    "    \n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w, tag) for (w, tag) in tagged_words]\n",
    "    \n",
    "    # map from original word to lemmatized\n",
    "    original_to_lemma = {(w.lower(), i) : lemmatizer.lemmatize(w, tag).lower() for (i, (w, tag)) in enumerate(tagged_words)}\n",
    "\n",
    "    \n",
    "    \n",
    "    # stopwords\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    punctuation = string.punctuation\n",
    "    normalized_words = map(lambda x: x.lower(), lemmatized_words)\n",
    "    clean_words = list(filter(lambda x: x not in english_stopwords and x not in punctuation, normalized_words))\n",
    "    \n",
    "    original_to_lemma = {k : v for k, v in original_to_lemma.items() if v not in english_stopwords and v not in punctuation}\n",
    "\n",
    "\n",
    "    # collocations\n",
    "    clean_words, original_to_lemma = wn_collocations(clean_words, original_to_lemma)\n",
    "\n",
    "    \n",
    "    #remove duplicates - order doesn't matter to pagerank\n",
    "    clean_words = list(set(clean_words))\n",
    "    \n",
    "    return clean_words, original_to_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-MTtvaWSUVKY"
   },
   "outputs": [],
   "source": [
    "def to_wordnet_pos(nltk_pos):\n",
    "    if nltk_pos == \"J\":\n",
    "        return wn.ADJ\n",
    "    elif nltk_pos == \"N\":\n",
    "        return wn.NOUN\n",
    "    elif nltk_pos == \"V\":\n",
    "        return wn.VERB\n",
    "    elif nltk_pos == \"R\":\n",
    "        return wn.ADV\n",
    "    elif nltk_pos == \"S\":\n",
    "        return wn.ADJ_SAT\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sPX85gNGUVKj"
   },
   "outputs": [],
   "source": [
    "def wn_collocations(words, original_to_lemma):\n",
    "    # for every col_size group of adjacent words check if they are a collocation\n",
    "    # i.e. there is a meaning in wordnet for the group\n",
    "    max_col_size = 5\n",
    "    for col_size in range(max_col_size, 1, -1):\n",
    "        to_delete = []\n",
    "        to_delete_o2l = []\n",
    "        cols_to_add = []\n",
    "        cols_to_add_ind = []\n",
    "        for (k, ix), v in original_to_lemma.items():\n",
    "            if ix > len(words) - col_size:\n",
    "                break\n",
    "            if ix in to_delete:\n",
    "                continue\n",
    "            clean_col = \"_\".join([words[j] for j in range(ix, ix + col_size)])\n",
    "            orig_col = \"_\".join([k for (k, index) in original_to_lemma.keys() if index in range(ix, ix + col_size)])\n",
    "\n",
    "            # if there is a meaning in wordnet - add collocation as a new 'word' and delete individual words\n",
    "            # perhaps too complicated way to do that, but if it's stupid and it works...\n",
    "            if len(wn.synsets(clean_col)) != 0:\n",
    "                cols_to_add.append((orig_col, clean_col))\n",
    "                cols_to_add_ind.append(ix)\n",
    "                \n",
    "                to_delete_o2l.append(ix)\n",
    "                words[ix] = clean_col\n",
    "\n",
    "                for j in range(ix + col_size - 1, ix, -1):\n",
    "                    to_delete.append(j)\n",
    "                    to_delete_o2l.append(j)\n",
    "                    \n",
    "        new_indexes = list(range(len(words) - len(to_delete_o2l)))\n",
    "        new_or_to_lemma = {}\n",
    "        new_ix = 0\n",
    "        help_i = 0\n",
    "        for (k, ix), v in original_to_lemma.items():\n",
    "            if ix not in to_delete_o2l:\n",
    "                new_or_to_lemma[(k, new_ix)] = v\n",
    "                new_ix += 1\n",
    "            elif ix in cols_to_add_ind:\n",
    "                new_or_to_lemma[(cols_to_add[help_i][0], new_ix)] = cols_to_add[help_i][1]\n",
    "                help_i += 1\n",
    "                new_ix += 1\n",
    "\n",
    "        original_to_lemma = new_or_to_lemma\n",
    "        words = [w for (i, w) in enumerate(words) if i not in to_delete]\n",
    "\n",
    "    original_to_lemma = {k : v for ((k, ix), v) in original_to_lemma.items()}\n",
    "\n",
    "\n",
    "    return words, original_to_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xOVOxnxUVKt"
   },
   "outputs": [],
   "source": [
    "def create_graph(clean_words, word_senses):\n",
    "    G = nx.Graph()\n",
    "    # create nodes\n",
    "    for w in clean_words:\n",
    "        # TODO: try adding pos when getting synsets\n",
    "        w_synsets = wn.synsets(w)\n",
    "        if len(w_synsets) > 1:\n",
    "            for synset in w_synsets:\n",
    "                G.add_node((w, synset))\n",
    "        elif len(w_synsets) == 1:\n",
    "            # if there is only one meaning in wordnet, we choose that one\n",
    "            word_senses[w] = w_synsets[0]\n",
    "    #     if there are no synsets - leave None\n",
    "    #                 G.add_node(synset.name())\n",
    "    \n",
    "    # create edges\n",
    "    for node_i in G.nodes:\n",
    "        for node_j in G.nodes:\n",
    "            if node_i[0] != node_j[0] and are_connected(node_i[1], node_j[1]):\n",
    "                G.add_edge(node_i, node_j)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAKR83XUUVK0"
   },
   "outputs": [],
   "source": [
    "def are_connected(u, v):\n",
    "    if u in v.hypernyms() or u in v.hyponyms() or u in v.part_holonyms() or u in v.part_meronyms() or u in v.substance_holonyms() or u in v.substance_meronyms():\n",
    "        return True\n",
    "#     coordinate relation - e.g. wolf and dog\n",
    "#     + this will connect two different words that are in the same synset = we think this should be done \n",
    "#     it is not specified in the original paper\n",
    "    u_hyper = set(u.hypernyms())\n",
    "    v_hyper = set(v.hypernyms())\n",
    "    if len(u_hyper & v_hyper) > 0:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adoPdlf2UVK8"
   },
   "outputs": [],
   "source": [
    "def evaluate(pr, word_senses, original_to_lemma, file):\n",
    "    max_values = dict([(w, 0.0) for w in word_senses.keys()])\n",
    "    \n",
    "    # for each word assign sense as the max value from pagerank\n",
    "    for k, v in pr.items():\n",
    "        if v > max_values[k[0]]:\n",
    "            word_senses[k[0]] = k[1]\n",
    "            max_values[k[0]] = v\n",
    "    \n",
    "    # get actual meaning of words from original semcor file\n",
    "    actual_meaning = get_actual_meaning(file)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = calculate_accuracy(original_to_lemma, word_senses, actual_meaning)\n",
    "    print('Accuracy: {}'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aoQlCykoUVLE"
   },
   "outputs": [],
   "source": [
    "def get_actual_meaning(file):\n",
    "    actual_meaning = {}\n",
    "    no_lemma_sense = 0\n",
    "    trees = list(semcor.tagged_chunks(fileids=file, tag='sem'))\n",
    "    no_trees = 0\n",
    "    for tree in trees:\n",
    "        if isinstance(tree, nltk.tree.Tree):\n",
    "            actual_meaning['_'.join(tree.leaves()).lower()] = tree.label()\n",
    "            if not isinstance(tree.label(), nltk.corpus.reader.wordnet.Lemma):\n",
    "                #TODO: handle these - create Lemma (or Synset) from string if possible...\n",
    "                # There are mistakes in semcor file: e.g. toe.a.00 sense doesn't exist in wordnet\n",
    "                no_lemma_sense += 1\n",
    "        else:\n",
    "          no_trees += 1\n",
    "    print('Number of words that do not have lemma sense in semcor file: {}'.format(no_lemma_sense))\n",
    "    print('Number of words that do not have tree in semcor file: {}'.format(no_trees))\n",
    "    \n",
    "    return actual_meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1sBceKrUVLN"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(original_to_lemma, word_senses, actual_meaning):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    broken_meaning = 0\n",
    "#     print(actual_meaning)\n",
    "    \n",
    "    for k, v in actual_meaning.items():\n",
    "        if not isinstance(v, nltk.corpus.reader.wordnet.Lemma):\n",
    "#             print(\"Broken meaning in the semcor file for: {}\".format(k))\n",
    "            broken_meaning += 1\n",
    "        elif k not in original_to_lemma.keys():\n",
    "            if len(wn.synsets(k)) != 0:\n",
    "                # they have a collocation that exists in wordnet, but we don't\n",
    "                # probably contains stopwords...\n",
    "#                 print(\"They found a collocation, and we didn't: {}\".format(k))\n",
    "                incorrect += 1\n",
    "        elif original_to_lemma[k] not in word_senses.keys():\n",
    "            print(\"This shouldn't happen: {}\".format(k))\n",
    "        elif word_senses[original_to_lemma[k]] is None:\n",
    "            # probably the word doesn't exist in wordnet...\n",
    "#             print(\"They have a meaning, and we don't: {}\".format(k))\n",
    "            incorrect += 1\n",
    "        elif v in word_senses[original_to_lemma[k]].lemmas():\n",
    "            # we chose the right meaning\n",
    "            correct += 1\n",
    "        else:\n",
    "            # we chose the wrong meaning\n",
    "            incorrect += 1\n",
    "\n",
    "\n",
    "    print(\"Correct = {}, incorrect = {}\".format(correct, incorrect))\n",
    "    \n",
    "    return correct / (correct + incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LI5_W7ksUVLW",
    "outputId": "aec84a0a-e89c-47c3-9646-3cf2bbe16eab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('nehru.n.01')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('jawaharlal_nehru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wceSGtWFUVLf",
    "outputId": "9f978765-502f-4cd5-fd15-8eb6d61282bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brownv/tagfiles/br-a10.xml', 'brownv/tagfiles/br-a08.xml', 'brownv/tagfiles/br-f12.xml', 'brownv/tagfiles/br-h02.xml', 'brownv/tagfiles/br-a34.xml']\n",
      "Number of words that do not have lemma sense in semcor file: 6\n",
      "Number of words that do not have tree in semcor file: 1807\n",
      "Correct = 148, incorrect = 16\n",
      "Accuracy: 0.9024390243902439\n",
      "Number of words that do not have lemma sense in semcor file: 5\n",
      "Number of words that do not have tree in semcor file: 1785\n",
      "Correct = 130, incorrect = 21\n",
      "Accuracy: 0.8609271523178808\n",
      "Number of words that do not have lemma sense in semcor file: 6\n",
      "Number of words that do not have tree in semcor file: 1979\n",
      "Correct = 141, incorrect = 21\n",
      "Accuracy: 0.8703703703703703\n",
      "Number of words that do not have lemma sense in semcor file: 6\n",
      "Number of words that do not have tree in semcor file: 1867\n",
      "Correct = 127, incorrect = 13\n",
      "Accuracy: 0.9071428571428571\n",
      "Number of words that do not have lemma sense in semcor file: 6\n",
      "Number of words that do not have tree in semcor file: 1872\n",
      "Correct = 129, incorrect = 20\n",
      "Accuracy: 0.8657718120805369\n",
      "Average wsd accuracy: 0.8813302432603777 achieved on 5 files\n"
     ]
    }
   ],
   "source": [
    "wsd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "43uLR1eeUVLq",
    "outputId": "fb9ace09-76e6-4bfe-c0e8-559f7a6a6fa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('sideline.n.01')]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collocations that contain stopwords\n",
    "wn.synsets('responsible_for')\n",
    "wn.synsets('out_of_bounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdFB_T0oUVLy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('eye-strain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLzENQdlUVL7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G53wQG1UVMC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRQE5OxCUVMJ"
   },
   "outputs": [],
   "source": [
    "# some testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBdhcl1dUVMO",
    "outputId": "b378c16d-abe4-4da9-b968-7490d9b75fda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('refuse', 'NN')]"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['refuse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVZQZ4v5UVMV",
    "outputId": "fb32cbd5-f406-4a0b-f79a-eb7d8e98bc8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('they', 'PRP'), ('refuse', 'VBP')]"
      ]
     },
     "execution_count": 123,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['they', 'refuse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2uPysq3RUVMb"
   },
   "outputs": [],
   "source": [
    "#pos tagging depends on context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ChzFYYKUVMl",
    "outputId": "14cbc4cf-4720-4da7-aed1-c1a433779651"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('are.n.01'),\n",
       " Synset('be.v.01'),\n",
       " Synset('be.v.02'),\n",
       " Synset('be.v.03'),\n",
       " Synset('exist.v.01'),\n",
       " Synset('be.v.05'),\n",
       " Synset('equal.v.01'),\n",
       " Synset('constitute.v.01'),\n",
       " Synset('be.v.08'),\n",
       " Synset('embody.v.02'),\n",
       " Synset('be.v.10'),\n",
       " Synset('be.v.11'),\n",
       " Synset('be.v.12'),\n",
       " Synset('cost.v.01')]"
      ]
     },
     "execution_count": 138,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('are') # there are stopwords in wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGOcFCt9UVM8",
    "outputId": "d7c3abc8-2ce8-4ce5-b844-81482d653976"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('russell.n.07')]"
      ]
     },
     "execution_count": 147,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bertrand_russell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tL8xPstdUVNB",
    "outputId": "e5ec12c6-fa78-4593-cadd-97fab0f4919b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('russell.n.07.Russell'),\n",
       " Lemma('russell.n.07.Bertrand_Russell'),\n",
       " Lemma('russell.n.07.Bertrand_Arthur_William_Russell'),\n",
       " Lemma('russell.n.07.Earl_Russell')]"
      ]
     },
     "execution_count": 148,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bertrand_russell')[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fq7vcUZgUVNI"
   },
   "outputs": [],
   "source": [
    "test_words = ['I', 'am', 'Bertrand', 'Arthur', 'William', 'Russell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EfMFTgo8UVNN"
   },
   "outputs": [],
   "source": [
    "pam = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztQhMTxsUVNT"
   },
   "outputs": [],
   "source": [
    "wn_collocations(test_words, pam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KAsNrDXZUVNa",
    "outputId": "24c35676-adc3-4522-8f6f-ac53a694e21e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'Bertrand_Arthur_William_Russell']"
      ]
     },
     "execution_count": 152,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7Gc7feiUVNq",
    "outputId": "27d05562-f244-4b25-fbf1-697297a0c4b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 199,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"n't\") # => errors in semcor file ???"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wsd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
